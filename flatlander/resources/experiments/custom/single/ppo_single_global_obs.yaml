# to check whether the filling obs can even be used to learn meaningful policies
flatland-sparse-small-tree-fc-ppo-single:
    run: PPO
    env: flatland_single
    stop:
        timesteps_total: 20000000 # 2e7
    checkpoint_freq: 10
    checkpoint_at_end: True
    keep_checkpoints_num: 5
    checkpoint_score_attr: episode_reward_mean
    config:
        num_workers: 7
        num_envs_per_worker: 5
        num_gpus: 1

        clip_rewards: False
        vf_clip_param: 2500
        entropy_coeff: 0.01
        # effective batch_size: train_batch_size * num_agents_in_each_environment [5, 10]
        # see https://github.com/ray-project/ray/issues/4628
        train_batch_size: 1000  # 5000
        rollout_fragment_length: 50  # 100
        sgd_minibatch_size: 100  # 500
        vf_share_layers: False

        env_config:
            observation: pi_global
            observation_config:
                max_width: 45
                max_height: 45
                max_n_agents: 5
                global_obs: True

            generator: sparse_rail_generator
            generator_config: small_v0_1agent

            wandb:
                project: flatland
                entity: wullli
                tags: ["small_v0", "tree_obs", "ppo, "custom", "single_agent"]

        model:
            custom_model: global_obs_model
            custom_model_config:
                architecture: nature



